---
layout: post
title:  "蒙特卡罗积分"
date:   2024-06-24 16:16:00
category: Math
---

- [(1) 蒙特卡罗法用到的数学理论知识](#1-蒙特卡罗法用到的数学理论知识)
  - [1.1 随机变量（Random Variable）](#11-随机变量random-variable)
  - [1.2 概率密度函数（Probability Density Function）](#12-概率密度函数probability-density-function)
  - [1.3 累积分布函数（Cumulative Distribution Function）](#13-累积分布函数cumulative-distribution-function)
  - [1.4 边缘密度函数（Marginal Density Function）与条件密度函数（Conditional Density Function）](#14-边缘密度函数marginal-density-function与条件密度函数conditional-density-function)
  - [1.5 期望值（Expected Value）](#15-期望值expected-value)
  - [1.6 方差（Variance）与标准差（Standard Deviation）](#16-方差variance与标准差standard-deviation)
  - [1.7 其它知识](#17-其它知识)
- [(2) 蒙特卡罗法](#2-蒙特卡罗法)
- [(3) 蒙特卡罗估计积分](#3-蒙特卡罗估计积分)
- [(4) 生成符合指定分布的随机数](#4-生成符合指定分布的随机数)
- [(5) 重要性采样](#5-重要性采样)
- [(6) 拟蒙特卡洛](#6-拟蒙特卡洛)

**蒙特卡罗积分（Monte Carlo Integration）** 是指使用 **蒙特卡罗法（Monte Carlo Methods）** 来估计积分。蒙特卡罗积分在图形渲染中起到非常重要的作用。

## (1) 蒙特卡罗法用到的数学理论知识

自然界中，有一类现象在一定条件下必然会发生，例如向上抛一石子、同性电荷互相排斥等，这类现象称为 **确定性现象** 。还有一类现象在大量重复试验下，呈现出某种规律（例如多次重复抛一枚硬币得到正面朝上大致有一半），这种大量重复试验中所呈现出的固有规律性，就是我们所说的 **统计规律性** 。这种在个别试验中结果呈现不确定性，在大量重复试验中结果有统计规律性的现象，就称之为 **随机现象** 。

在随机试验中，尽管在每次试验之前不能预知结果，但试验的所有可能结果组成的集合是已知的，我们将随机试验的所有可能结果组成的集合称为 **样本空间（Sample Space）** ，样本空间通常用 $S$ 来表示，例如在投骰子的情况下，样本空间被定义为 $S = \lbrace 1,2,3,4,5,6 \rbrace$ ，样本空间的元素称为 **样本点** 。 $\{123\}$

在相同的条件下，进行了 $n$ 次试验，在这 $n$ 次试验中，事件 A 发生的次数 $n_A$ 称为事件 A 发生的 **频数** ，比值 $n_A/n$ 称为事件 A 发生的 **频率** 。当重复试验的次数 $n$ 逐渐增大时，频率呈现出稳定性，逐渐稳定于某个常数，这种“频率稳定性”即通常所说的统计规律性，用来表示事件 A 发生可能性的大小，这就是所谓的 **概率** ，通常用 $P$ 来表示。

### 1.1 随机变量（Random Variable）

**随机变量** 用于描述随机试验中可能的结果，通常使用大写字母来表示，例如 $X$ 、 $Y$ 等。 **随机变量是一个函数，将每一个可能的结果映射到一个实数** 。以抛硬币为例，它的样本空间是 $S = \lbrace 正面, 反面 \rbrace$ ，那么抛硬币的随机变量可以定义为： $X(正面) = 1$ ， $X(反面) = 0$ 。

随机变量有两种类型： **离散（Discrete）随机变量** 和 **连续（Continuous）随机变量** 。当随机变量是离散型时，随机过程的结果只能取一列确切的值。掷骰子、抛硬币都是只能用离散随机变量来衡量的试验的例子，因为它们产生 **有限** 或 **可数无限** 的多个值。温度测量可以被视为连续型随机变量的一个例子，因为测量温度的试验可以取得的可能值落在一个连续区间范围内，无法列举所有的可能性。

### 1.2 概率密度函数（Probability Density Function）

概率密度函数，简称为 PDF ，是用于描述 **连续型随机变量的概率分布的函数** 。 **PDF 的值表示的是连续型随机变量在某个确定的取值点附近的可能性大小** 。对于一个连续型随机变量 $X$ ，概率密度函数 $p(x)$ 满足：

- 非负性：对于所有的 $x$ ，有 $p(x) \geq 0$
- 归一化：整个实数集上的积分等于 1，即 $\int_{-\infty}^{+\infty} p(x) \mathrm{d}x = 1$
- 对于任意两个实数 $a$ 和 $b$ ，随机变量 $X$ 落在区间 $[a,b]$ 内的概率可以通过积分计算： $P(a \leq X \leq b) = \int_{a}^{b} p(x) \mathrm{d}x$

### 1.3 累积分布函数（Cumulative Distribution Function）

累积分布函数，简称为 CDF 。对于一个随机变量 $X$ ，其 CDF $F(x)$ 定义为 $F(x) = P(X \leq x)$ ，它描述的是 **随机变量 $X$ 取值小于或等于某个特定值 $x$ 的累积概率** 。对于离散型随机变量， CDF 是一个阶梯函数；对于连续型随机变量， CDF 是一个连续函数。

对于连续型随机变量，通过对其 PDF 进行积分可以得到 CDF ： $F(x) = \int_{-\infty}^{x} p(x) \mathrm{d}x$ ；通过对 CDF 进行求导，可以得到 PDF ： $p(x) = \frac{\mathrm{d}}{\mathrm{d}x} F(x)$ 。

对于离散型随机变量，没有概率密度函数的概念。但可以通过概率质量函数（Probability Mass Function, PMF）来描述概率分布。在这种情况下， CDF 是通过累加 PMF 的值来得到的。

### 1.4 边缘密度函数（Marginal Density Function）与条件密度函数（Conditional Density Function）

边缘密度函数用于描述 **一个多元随机变量中的某个单一随机变量的概率分布，而忽略其它随机变量的影响** 。假设有两个连续型随机变量 $X$ 和 $Y$ ，它们的 **联合概率密度函数（Joint Probability Density Function, Joint PDF）** 为 $p(x,y)$ 。那么，随机变量 $X$ 的边缘密度函数 $p_X(x)$ 可以通过对 $Y$ 的整个取值范围积分来得到：

$$ p_X(x) = \int_{-\infty}^{\infty} p(x,y) \mathrm{d}y $$

类似的，随机变量 $Y$ 的边缘密度函数 $p_Y(y)$ 可以通过对 $X$ 的整个取值范围积分来得到：

$$ p_Y(y) = \int_{-\infty}^{\infty} p(x,y) \mathrm{d}x $$

条件密度函数描述的是 **一个随机变量在另一个随机变量已知的条件下的概率分布** 。还是上面边缘密度函数的例子，在给定 $Y = y$ 的条件下， $X$ 的条件密度函数 $p_{X \| Y}(x \| y)$ 定义为：

$$ p_{X | Y}(x | y) = \frac{p(x,y)}{p_Y(y)} $$

其中， $p_Y(y)$ 是 $Y$ 的边缘密度函数，前提是 $p_Y(y) > 0$ 。

类似的，在给定 $X = x$ 的条件下， $Y$ 的条件密度函数 $p_{Y \| X}(y \| x)$ 定义为：

$$ p_{Y | X}(y | x) = \frac{p(x,y)}{p_X(x)} $$

其中， $p_X(x)$ 是 $X$ 的边缘密度函数，前提是 $p_X(x) > 0$ 。

### 1.5 期望值（Expected Value）

如果从随机变量中反复抽取样本，所获得的平均值称为 **期望值** ，使用符号 $E$ 来表示。具体来说， **期望值是对随机变量取值的加权平均，权重是每个可能值的概率** ，它提供了一个关于随机变量取值的集中趋势的度量。

对于一个随机变量 $X$ ，它从具有 $n$ 个离散值 $x_i$ 的样本空间中随机抽取结果，样本空间中每个离散值的概率密度是 $p_i$ 。那么，期望值 $E[X]$ 可以表示为：

$$ E[X] = \sum_{i=1}^{n} x_i p_i $$

对于 **连续型随机变量** $X$ ：

$$ X \sim p(x) $$

它的概率密度函数满足：

$$ p(x) \geq 0 $$

并且将连续区间内的概率密度积分起来结果为 1 ：

$$ \int p(x)\mathrm{d}x = 1 $$

连续型随机变量 $X$ 的期望值 $E[X]$ 是：

$$
E[X] = \int x p(x) \mathrm{d}x
$$

随机变量 $X$ 的函数 $Y$ 也是一个随机变量：

$$ X \sim p(x) $$

$$ Y = f(X) $$

函数 $Y$ 的期望可以表示为：

$$ E[Y] = E[f(X)] = \int f(x)p(x) \mathrm{d}x $$

### 1.6 方差（Variance）与标准差（Standard Deviation）

**方差** 是用来描述一个随机变量的离散程度。对于一个随机变量 $X$ ，其期望值 $E[X] = \mu$ ，方差表示的是 **随机变量 $X$ 与其期望值 $E[X]$ 之间差值的平方的期望** ：

$$ \text{Var}(X) = E[(X - \mu)^2] $$

方差的另一个常用公式是：

$$ \text{Var}(X) = E[X^2] - (E[X])^2 $$

这个公式有时在实际计算中更为方便。

对于离散型随机变量，方差可以表示为：

$$ \text{Var}(X) = \sum_{i} (x_i - \mu)^2 \cdot P(X = x_i) $$

对于连续型随机变量，方差可以表示为：

$$ \text{Var}(X) = \int (x - \mu)^2 \cdot p(x)\mathrm{d}x $$

其中， $p(x)$ 是随机变量 $X$ 的概率密度函数。

**标准差** 是方差的平方根，使用 $\sigma$ 表示，定义如下：

$$ \sigma = \sqrt{\text{Var}(X)} $$

### 1.7 其它知识

设随机变量 $X$ 具有数学期望 $E[X] = \mu$ ，方差 $\text{Var}(X) = \sigma^2$ ，则对于任意实数 $k > 0$ ，不等式

$$ P(|X - \mu| \geq k) \leq \frac{\sigma^2}{k^2} $$

成立，这一不等式称为 **切比雪夫不等式（Chebyshev's inequality）** 。

**辛钦大数定理**：设 $\lbrace X_1, X_2, \ldots \rbrace$ 是相互独立，并且服从统一分布的随机变量序列，每个随机变量的期望值为 $E[X_i] = \mu$ 。那么对于任意的 $\epsilon > 0$ ，有：

$$ \lim_{n \to \infty} P\left(\left| \frac{1}{n} \sum_{i=1}^{n} X_i - \mu \right| < \varepsilon \right) = 1 $$

**通过辛钦大数定理可以知道，对于足够大的样本数量，样本平均值将非常接近于总体的数学期望** 。

**伯努利大数定理**：设 $\lbrace X_1, X_2, \ldots, X_n \rbrace$ 是相互独立，并且服从统一分布的随机变量序列，每个 $X_i$ 取值为1（表示成功）或0（表示失败），成功的概率为 $p$（即 $E[X_i] = p$ ）。定义总成功次数为 $S_n = X_1 + X_2 + \cdots + X_n$ ，则相对频率为 $\frac{S_n}{n}$ 。伯努利大数定理表述为：

$$ \lim_{n \to \infty} P\left(\left| \frac{S_n}{n} - p \right| < \varepsilon \right) = 1 $$

**伯努利大数定理说明了在大量独立重复试验中，经验概率会收敛于理论概率** 。

## (2) 蒙特卡罗法

蒙特卡罗法是一类通过随机采样来求解问题的算法的统称，要求解的问题是某随机事件的概率或某随机变量的期望。通过随机抽样的方法，以随机事件出现的频率估计其概率，并将其作为问题的解。

![montecarlo2](/assets/images/2024/2024-09-24-MonteCarloIntegration/montecarlo2.png)

蒙特卡洛的基本做法是通过大量重复试验，通过统计频率，来估计概率，从而得到问题的求解。如上图所示，一个矩形内有一个不规则图案，要求解不规则图形的面积。显然，矩形的面积可以简单计算为 $A = ab \times ac$ ，点位于不规则形状内的概率为 $p = \frac{A_{shape}}{A}$ ，现在重复往矩形范围内随机的投射点，样本点有一定概率会落在不规则图形内，若复杂 $n$ 次试验，落到不规则图形内的次数为 $k$ ，频率为 $\frac{k}{n}$ ，若样本数量较大，则有：

$$ p = \frac{A_{shape}}{A} \approx \frac{k}{n} $$

根据伯努利大数定理得到的结论就是：随着样本数增加，频率 $\frac{k}{n}$ 会收敛于概率 $p$ ，使得该等式成立。

因此，我们可以估计出不规则图形的面积为 $A_{shape} = \frac{k}{n} A$ 。假设矩形面积为1，投射了1000次，有200个点位于不规则形状内，则可以推算出不规则图形的面积为0.2，投射的次数越大，计算出来的面积越精确。

这样的一个例子说明蒙特卡洛方法的基本思路，它并不是“缘分”求解法，而是有严格的数学基础作为依托，前面介绍的大数定理是它重要的理论基础。但是，蒙特卡洛方法的求解的结果是有误差的，重复的试验越多误差就会越低。

## (3) 蒙特卡罗估计积分

举一个简单的例子，设一个函数 $f(x) = 3x^2$ ，计算其在区间 $[a,b]$ 上的积分值，如下图所示，容易得到：

$$ F(x) = \int_{a}^{b} f(x) = x^3 \Big|_{a}^{b} $$

![function_int](/assets/images/2024/2024-09-24-MonteCarloIntegration/function_int.jpeg)

假定要求的积分区间是 $[1,3]$ ， 那么积分结果为 $3^3 - 1^3 = 26$ 。

若采用蒙特卡罗方法来估计积分。先给出蒙特卡罗积分的一般等式，设 $\lbrace X_1, X_2, \ldots, X_n \rbrace$ 是相互独立的样本且服从同一分布，概率密度函数为 $p(x)$ ，则使用蒙特卡罗法积分可以表示为：

$$ F_n(X) = \frac{1}{n}\sum_{k=1}^{n} \frac{f(X_k)}{p(X_k)}$$

回到上面那个例子，函数 $f(x)$ 在区间 $[a,b]$ 上均匀分布，则任意一个样本点的概率密度函数是一个常数，为 $p(x) = \frac{1}{b - a}$ ，带入上面的等式可知：

$$ F_n(X) = \frac{b-a}{n} \sum_{k=1}^{n}f(X_k) $$

下面来证明蒙特卡罗法的积分估计量的正确性：

$$
\begin{align*}
E[F_N] &= E\Big[\frac{1}{N} \sum_{k=1}^{N} \frac{f(X_i)}{p(X_i)}\Big]\\
&= \frac{1}{N} \sum_{k=1}^{N} E\Big[\frac{f(X_i)}{p(X_i)}\Big]\\
&= \frac{1}{N} \sum_{k=1}^{N} \int_{a}^{b} \frac{f(x)}{p(x)} p(x) \mathrm{d}x\\
&= \frac{1}{N} \sum_{k=1}^{N} \int_{a}^{b} f(x)\mathrm{d}x\\
&= \int_{a}^{b} f(x)\mathrm{d}x
\end{align*}
$$

从上面的推导可以看出蒙特卡洛法的积分估计量的数学期望等于被积函数的积分真值，证明 $F_n(X)$ 是 **无偏估计量** 。

## (4) 生成符合指定分布的随机数

<!-- 随机变量 $X$ 通常表示某种概率分布，随机数通常指生成某种概率分布的生成器，也就是随机变量 $X$ 的生成器。如何生成符合指定概率分布特点的随机数就是下面要介绍的内容。 -->

设 $X$ 是一个随机变量，它的概率密度函数为 $p(x)$ ，它的累积分布函数可以表示为：

$$ cdf(x) = \int_{-\infty}^{+\infty} p(t) \mathrm{d}t $$

那么，计算符合该概率分布的随机数方法如下：

1. 对概率密度函数 $p(x)$ 进行积分计算出累积分布函数 $cdf(x)$
2. 计算 $cdf(x)$ 的反函数 $cdf^{-1}(x)$
3. 对于一个均匀分布的随机数 $\xi$ ， 则 $cdf^{-1}(\xi)$ 就是符合该概率分布的随机数

举个例子，已知区间 $[0,1]$ 之间的概率密度函数：

$$ p(x) = cx^n , x \in [0, 1] $$

其中 $c$ 和 $n$ 是一个常数，由于概率密度函数要求 $\int_{0}^{1} p(x) = 1$ ，则：

$$ \int_{0}^{1} cx^n = c\frac{x^{n+1}}{n+1} \Big|_{0}^{1} = \frac{c}{n+1} = 1 $$

得到 $c = n+1$ ，首先计算累积分布函数：

$$ cdf(x) = \int_{0}^{x} (n+1)t^n \mathrm{d}t = x^{n+1} $$

其反函数为：

$$ cdf^{-1}(x) = \sqrt[n+1]{x} $$

则符合该概率分布的随机数为：

$$ X = \sqrt[n+1]{\xi} $$

## (5) 重要性采样

**重要性采样（Importance Sampling）** 是已知被积函数的一些分布信息而采用的一种缩减方差的策略，还有别的策略像 **俄罗斯轮盘切割（Russian Roulette and Splitting）** ， **分层采样（Stratified Sampling）** ， **拉丁超立方体采样（Latin Hypercube Sampling）** 等，都是通过控制采样的策略达到缩减方差的目的。

考虑一个简单的例子，设一个函数为：

$$
f(x)=\begin{cases}
    99.01 & x \in [0, 0.01) \\
    0.01 & x \in [0.01, 1]
\end{cases}
$$

它的几何示意图如下图所示：

![00_mc](/assets/images/2024/2024-09-24-MonteCarloIntegration/00_mc.jpeg)

我们采用蒙特卡洛法估计它的积分，选择区间 $[0, 1]$ 之间的均匀分布作为它的随机数，那么存在绝大部分的采样点在区间 $[0.01, 1]$ 之间，但是它对积分估计的贡献只有0.01，小部分采样点在区间 $[0, 0.01)$ 之间，它对积分估计的贡献却非常的大，这种现象就会导致误差非常的大，简单提高采样数对估计量收敛的影响较小。

蒙特卡洛法的核心，是根据某一概率分布来随机采样，那么，**根据被积函数曲线规律来设计类似的概率密度来进行采样，这就符合蒙特卡罗积分的要求，也就是重要性采样的核心**。

考虑另外一个例子，需要计算函数 $f(x) = \sin{x}, x \in [0, \frac{\pi}{2}]$ 的积分。首先，很容易计算出它的积分真值为：

$$ F(x) = \int_{0}^{\frac{\pi}{2}} f(x) \mathrm{d}x = -\cos{x} \Big|_{0}^{\frac{\pi}{2}} = 1 $$

我们设计两种概率密度函数，一种是均匀分布，随机变量 $X_1$ 的概率密度函数为：

$$ p_1(x) = \frac{1}{\frac{\pi}{2}}, x \in [0, \frac{\pi}{2}] $$

另外一种，随机变量 $X_2$ 概率密度函数为：

$$ p_2(x) = \frac{8x}{\pi^2}, x \in [0, \frac{\pi}{2}] $$

两种概率分布与函数的几何示意图如下图所示：

![01_mc](/assets/images/2024/2024-09-24-MonteCarloIntegration/01_mc.jpeg)

显然 $p_2(x)$ 更符合被积函数的曲线特点。

## (6) 拟蒙特卡洛

**拟蒙特卡洛（Quasi Monte Carlo）积分** 估计技术的核心点是积分估计时采用 **低差异序列（Low Discrepancy Sequence）** 来替换纯随机数，它的优点是积分估计的收敛速度更快，理论上拟蒙特卡洛能达到 $O(n^{-1})$ 的收敛速度，而普通蒙特卡洛的收敛速度是 $O(n^{-0.5})$ ，更快的收敛速度意味着相同的采样数下，低差异序列估计的结果误差更小。

我们先举一个简单的例子解释，随机采样很可能会导致采样点的 **聚集（Clump）** 现象，如下图所示，聚集会导致积分估计的误差扩大：

![03_mc](/assets/images/2024/2024-09-24-MonteCarloIntegration/03_mc.jpeg)

为了消除这种聚集现象，就有了 **分层采样（Stratified Sampling）** 的提出，它是将采样空间均分为 $n$ 个格子，每个格子里面再随机采样一个点，如下图所示：

![04_mc](/assets/images/2024/2024-09-24-MonteCarloIntegration/04_mc.jpeg)

那么，有没有一种采样方式，它不需要对采样空间进行手动分割，就同时具备空间平均性和随机性这两个特点呢？数学家引入了 **差异（Discrepancy）** 这个概念，完全随机的样本具有很高的差异，但是完全平均的样本的差异为0，我们的目标就是找到 **一组有较低差异且不失随机性的序列，这就是低差异序列** ，以期望达到消除聚集的目的。

下面给出差异的标准数学定义：设 $P = \{ x_1, x_2, \cdots ,x_n \}$ 是一组位于空间 $[0,1]^s$ 下的点集，差异可以定义为：

![05_mc](/assets/images/2024/2024-09-24-MonteCarloIntegration/05_mc.jpeg)

其中， $s$ 表示维度， $\lambda_s$ 是 $s$ 维空间的体积度量（非标准命名）， $A(B)$ 表示点集 $P$ 中落入区域 $B$ 的点的个数， $J$ 表示 $s$ 维空间下的任意盒状包围范围。举个例子解释下差异的定义，如下图所示，在二维空间下区间 $[0,1] \times [0, 1]$ 内有一组采样点集，点集个数为 $n$ ，整个点集的面积（在二维空间下的体积度量就称为面积）就为 1， 任意划定一块区域 $B$ ，区域 $B$ 囊括了 $k$ 个采样点，此区域的面积是 $\lambda(B)$ ，那么差异指的就是点集个数比值 $k/n$ 与面积比值 $\lambda(B) / 1$ 的差的绝对值，注意B是指空间范围 $[0,1] \times [0, 1]$ 内的任意一块区域：

![06_mc](/assets/images/2024/2024-09-24-MonteCarloIntegration/06_mc.jpeg)

常用到的几种低差异序列有：

- Van der Corput序列；
- Halton序列；
- Hammersley序列；
- Sobol序列；

下面介绍下 Van der Corput序列、Halton序列和Hammersley序列。

Van der Corput序列是数学家Van der Corput在1935年设计的一种一维低差异序列，也是最简单的一种序列。现实生活中的数字通常都是十进制，例如1314可以拆解为：

$$ 1 \times 10^3 + 3 \times 10^2 + 1 \times 10^1 + 1 \times 10^0 $$

由十进制扩展到 $b$ 进制，它的数学表示就是：

$$ x = \sum_{k=0}^{m} d_k \cdot b^k $$

其中， $b$ 是底数， $d_k \in [0, b-1]$ 。

它对应的Van der Corput序列就可以表示为：

$$ g_b(x) = \sum_{k=1}^{m} d_k \cdot b^{-k-1} $$

生成步骤可以描述为：

1. **表示为基 \( b \) 的数字**：将 \( n \) 表示为基 \( b \) 的数字。例如，基 \( b = 2 \) 时，数字 2 表示为 10。
2. **反转数字顺序**：将这些数字的顺序反转。例如，10 反转后得到 01。
3. **转换为小数**：将反转后的数字作为小数的个位、十位、百位等。例如，01 变为 0.01（二进制），即 $2^{-2} = 0.25$ （十进制）。

以十进制数来说，1到13的Van der Corput序列就是：

$$ \{ \frac{1}{10},\frac{2}{10},\frac{3}{10},\frac{4}{10},\frac{5}{10},\frac{6}{10},\frac{7}{10},\frac{8}{10},\frac{9}{10},\frac{1}{100},\frac{11}{100},\frac{21}{100},\frac{31}{100} \} $$

以十进制的 10 来举个例子：

- 基 \( b = 10 \) 时，数字 10 表示为 10
- 反转数字顺序，得到 01
- 转换为小数，0.01
- 所以当基数为10时，十进制的10转换为Van der Corput序列就是 0.01

Halton序列就是将Van der Corput序列扩展为N维的情况，可以表示为：

$$ halton(x) = (g_{b_1}(x),g_{b_2}(x), \cdots , g_{b_N}(x)) $$

其中,底数 $b_1,b_2, \cdots, b_N$ 互为质数。

Hammersley序列是在已知采样点集个数的情况下才可计算出的序列，设采样点集个数为n，维度为N，Hammersley序列表示为：

$$ Hammersley(x) = (\frac{x}{n},g_{b_1}(x),g_{b_2}(x), \cdots , g_{b_{N-1}}(x)) $$

Halton序列的收敛速度是 $O((\log{n})^N / n)$ ，而Hammersley序列的收敛速度是 $O((\log{n})^{N-1} / n)$ ，所以在已知样本个数的情况下，Hammersley序列会有更优的表现。
